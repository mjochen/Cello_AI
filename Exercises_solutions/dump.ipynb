{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e55416",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89e4e17d",
   "metadata": {},
   "source": [
    "Binary Classifier Models for 103 Features\n",
    "\n",
    "With 103 input features, you can choose from a wide range of binary classification models depending on your goals (speed, interpretability, performance, etc.).\n",
    "ðŸ”¢ Linear Models\n",
    "\n",
    "    Logistic Regression\n",
    "\n",
    "        Fast, interpretable.\n",
    "\n",
    "        Good if features are not highly correlated and mostly linearly separable.\n",
    "\n",
    "        Works well even with many features if regularization is used (L1 or L2).\n",
    "\n",
    "ðŸŒ³ Tree-Based Models\n",
    "\n",
    "    Decision Tree\n",
    "\n",
    "        Easy to interpret, but prone to overfitting on high-dimensional data.\n",
    "\n",
    "    Random Forest\n",
    "\n",
    "        Handles many features well, reduces overfitting via ensembling.\n",
    "\n",
    "    Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost)\n",
    "\n",
    "        Among the best performers for tabular data.\n",
    "\n",
    "        Handles feature importance automatically and tolerates irrelevant features.\n",
    "\n",
    "        Often a top pick in Kaggle competitions.\n",
    "\n",
    "ðŸ“ˆ Linear + Kernel Methods\n",
    "\n",
    "    Support Vector Machine (SVM)\n",
    "\n",
    "        Linear kernel: efficient for many features.\n",
    "\n",
    "        RBF/kernel: powerful but slower, and sensitive to scaling/noise.\n",
    "\n",
    "ðŸ§  Neural Networks\n",
    "\n",
    "    MLPClassifier (Multi-layer Perceptron)\n",
    "\n",
    "        From scikit-learn, or build with PyTorch/TensorFlow.\n",
    "\n",
    "        Needs normalization and careful tuning.\n",
    "\n",
    "        Performs better with large amounts of training data.\n",
    "\n",
    "ðŸ“‰ Naive Bayes\n",
    "\n",
    "    GaussianNB / BernoulliNB\n",
    "\n",
    "        Very fast and simple.\n",
    "\n",
    "        Assumes feature independence â€” not ideal with many correlated features, but surprisingly effective in some cases.\n",
    "\n",
    "ðŸ§ª When to Use What?\n",
    "Scenario\tRecommended Model(s)\n",
    "Quick baseline\tLogistic Regression, Random Forest\n",
    "High accuracy needed\tXGBoost, LightGBM\n",
    "High-dimensional + linear\tLinear SVM, Logistic Regression\n",
    "Many irrelevant/noisy features\tRandom Forest, XGBoost\n",
    "Want feature importances\tRandom Forest, Logistic Regression (L1), XGBoost\n",
    "Limited training data\tNaive Bayes, Logistic Regression\n",
    "Large dataset + compute\tNeural Network, XGBoost\n",
    "\n",
    "You can start with Logistic Regression for a baseline, and move to Random Forest or XGBoost for stronger performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f543858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-label Classification Metrics: Macro vs. Micro Averages\n",
    "\n",
    "## ðŸ§ª Yeast Dataset Overview\n",
    "\n",
    "- **Samples**: 2,417  \n",
    "- **Features**: 103  \n",
    "- **Labels**: 14 functional classes  \n",
    "- **Multi-label nature**: Each sample may belong to multiple classes simultaneously.\n",
    "\n",
    "This dataset is ideal for studying multi-label classification metrics like **sensitivity (recall)** and **specificity**, especially for comparing **macro** and **micro** averages.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Why Use the Yeast Dataset?\n",
    "\n",
    "- **Label Imbalance**: Some classes are more prevalent than others, so it helps demonstrate how macro and micro averages respond to imbalanced data.\n",
    "- **Real-world Multi-label Structure**: Common in domains like bioinformatics and text classification.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Dataset Links\n",
    "\n",
    "- **Scikit-learn Example**:  \n",
    "  [Yeast Classifier Chain](https://scikit-learn.org/stable/auto_examples/multioutput/plot_classifier_chain_yeast.html?utm_source=chatgpt.com)\n",
    "\n",
    "- **Kaggle Notebook**:  \n",
    "  [Multi-label Classification with Scikit-learn](https://www.kaggle.com/code/roccoli/multi-label-classification-with-sklearn?utm_source=chatgpt.com)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® Code Example: Macro vs. Micro Recall\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Assuming y_true and y_pred are your true and predicted label matrices\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro')\n",
    "recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "\n",
    "print(f\"Macro-average Recall: {recall_macro:.3f}\")\n",
    "print(f\"Micro-average Recall: {recall_micro:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
